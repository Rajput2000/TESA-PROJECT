# ðŸ“š Library Assistant Chatbot ðŸ¤–

This is an intelligent chatbot that serves as a **Library Assistant**, built using **Streamlit** and integrated with a backend LLM model. It helps users interact with a digital library assistant through either a direct chat interface or an API-powered setup.

---

## ðŸš€ Features

- Conversational assistant powered by an LLM.
- Clean and interactive UI using **Streamlit**.
- User session management.
- Resettable chat history.
- Dual setup options: Standalone Streamlit app or FastAPI-based backend.

---

## ðŸ“‚ Project Structure

```
TESA-PROJECT/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                # Main Streamlit App
â”‚   â””â”€â”€ service.py            # ChatBot class (handles backend logic)
â”‚
â”œâ”€â”€ fast_api/
â”‚   â”œâ”€â”€ backend.py            # FastAPI backend with LLM endpoint
â”‚   â””â”€â”€ fast_app_front_end.py # Streamlit frontend that consumes FastAPI
â”‚
â””â”€â”€ README.md
```

---

## ðŸ§  Running the Streamlit App (Standalone)

You can try the app live here:  
ðŸ‘‰ [https://tesa-project-chat-bot.streamlit.app/](https://tesa-project-chat-bot.streamlit.app/)

Or run it locally with:

```bash
cd src
streamlit run app.py
```

---

## âš¡ Alternative Setup with FastAPI Backend

Use this method if you want to run the chatbot with a FastAPI backend that serves responses via API.

### Step 1: Start FastAPI Backend

```bash
cd fastapi
uvicorn backend:app --reload
```

This will start the backend at `http://127.0.0.1:8000`.

### Step 2: Start Streamlit Frontend (FastAPI Client)

In another terminal:

```bash
streamlit run fast_app_front_end.py
```

---

## ðŸ›  Requirements

- Python 3.8+
- Streamlit
- FastAPI
- Uvicorn
- Any dependencies used in your `service.py` or LLM model

Install them using:

```bash
pip install -r requirements.txt
```

---

## ðŸ“¬ Feedback

For issues or contributions, feel free to open a pull request or raise an issue on the [GitHub repo](https://github.com/Rajput2000/TESA-PROJECT/tree/main/Chat_Bot).

---
